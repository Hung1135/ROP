import re
import os
import logging
import pdfplumber
from docx import Document

from pdfminer.pdfparser import PDFSyntaxError
from pdfminer.pdfdocument import PDFTextExtractionNotAllowed
from pdfminer.psparser import PSEOF

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity


# ================== LOGGING ==================
logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s:%(name)s:%(message)s"
)
logger = logging.getLogger(__name__)


# ================== MODEL ==================
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")


# ================== CV EXTRACT ==================
def extract_text_from_pdf(file_path: str) -> str:
    if not os.path.exists(file_path):
        logger.error("‚ùå PDF kh√¥ng t·ªìn t·∫°i: %s", file_path)
        return ""

    if os.path.getsize(file_path) == 0:
        logger.error("‚ùå PDF r·ªóng (0 bytes): %s", file_path)
        return ""

    text = ""
    try:
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
    except (PDFSyntaxError, PDFTextExtractionNotAllowed, PSEOF) as e:
        logger.error("‚ùå PDF l·ªói c·∫•u tr√∫c (%s): %s", file_path, e)
        return ""
    except Exception as e:
        logger.exception("‚ùå L·ªói ƒë·ªçc PDF (%s): %s", file_path, e)
        return ""

    return text.strip()


def extract_text_from_docx(file_path: str) -> str:
    try:
        doc = Document(file_path)
        return "\n".join(p.text for p in doc.paragraphs).strip()
    except Exception as e:
        logger.exception("‚ùå L·ªói ƒë·ªçc DOCX (%s): %s", file_path, e)
        return ""


def extract_cv_text(file_path: str) -> str:
    if not file_path:
        return ""

    file_path = file_path.lower()
    if file_path.endswith(".pdf"):
        return extract_text_from_pdf(file_path)
    if file_path.endswith(".docx"):
        return extract_text_from_docx(file_path)

    logger.warning("‚ö†Ô∏è ƒê·ªãnh d·∫°ng CV kh√¥ng h·ªó tr·ª£: %s", file_path)
    return ""


# ================== TEXT PROCESSING ==================
def clean_text(text: str) -> str:
    if not text:
        return ""
    text = text.lower()
    text = re.sub(r"[^\w\s\.\+\#]", " ", text, flags=re.UNICODE)
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def tokenize(text: str) -> set:
    text = clean_text(text)
    return set(re.findall(r"\w+", text, flags=re.UNICODE))


def jaccard(a: set, b: set) -> float:
    if not a or not b:
        return 0.0
    return len(a & b) / len(a | b)


def text_to_vector(text: str):
    return model.encode(text)


def cosine_score(vec1, vec2) -> float:
    return float(cosine_similarity([vec1], [vec2])[0][0])


# ================== SPLIT CV & JD ==================
def split_cv_lines(cv_text: str) -> list:
    raw_parts = re.split(r"\n|‚Ä¢|-|‚Äì|‚Äî|;\s|\. ", cv_text)
    parts = []

    for p in raw_parts:
        p = clean_text(p)
        if not p:
            continue

        # b·ªè nhi·ªÖu
        if re.search(r"\b\d{9,}\b", p):  # phone / id
            continue
        if re.search(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}", p):
            continue
        if len(p.split()) < 3:
            continue

        parts.append(p)

    return parts


def split_jd_items(text: str) -> list:
    if not text:
        return []

    items = [
        clean_text(x)
        for x in re.split(r"\n|‚Ä¢|-|‚Äì|‚Äî|;\s|\. ", text)
        if clean_text(x)
    ]

    return [i for i in items if len(i.split()) >= 2]


# ================== MATCHING ==================
def match_list_score(cv_text, jd_text, section_name="Section", threshold=0.5) -> float:
    cv_lines = split_cv_lines(cv_text)
    jd_items = split_jd_items(jd_text)

    if not cv_lines or not jd_items:
        logger.info("[%s] Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ so kh·ªõp", section_name)
        return 0.0

    # üî• T·ªêI ∆ØU: encode CV 1 l·∫ßn
    cv_vectors = {line: text_to_vector(line) for line in cv_lines}

    kept_scores = []

    for item in jd_items:
        item_vec = text_to_vector(item)
        best_score = -1
        best_line = None

        for line in cv_lines:
            raw_cos = cosine_score(cv_vectors[line], item_vec)
            overlap = jaccard(tokenize(line), tokenize(item))
            combined = 0.7 * raw_cos + 0.3 * overlap

            if combined > best_score:
                best_score = combined
                best_line = line

        logger.debug(
            "[%s] JD: %s\n    CV: %s\n    Score: %.2f",
            section_name, item, best_line, best_score
        )

        if best_score >= threshold:
            kept_scores.append(best_score)

    return round((sum(kept_scores) / len(kept_scores)) * 100, 2) if kept_scores else 0.0


def match_cv_with_job_advanced(cv_text, job):
    if not cv_text:
        return 0, 0, 0, "Th·∫•p"

    logger.info("===== CV CONTENT =====\n%s", cv_text)
    logger.info(
        "===== JOB =====\nTitle: %s\nDesc: %s\nReq: %s\nSkill: %s\nBenefit: %s",
        job.title,
        job.description or "",
        job.requirements or "",
        job.skills or "",
        job.benefit or ""
    )

    req_score = match_list_score(cv_text, job.requirements or "", "Requirement", 0.5)
    skill_score = match_list_score(cv_text, job.skills or "", "Skill", 0.5)
    desc_score = match_list_score(cv_text, job.description or "", "Description", 0.45)

    ai_score = round(0.6 * skill_score + 0.3 * req_score + 0.1 * desc_score, 2)

    if ai_score >= 80:
        level = "R·∫•t ph√π h·ª£p"
    elif ai_score >= 60:
        level = "Ph√π h·ª£p"
    elif ai_score >= 40:
        level = "Trung b√¨nh"
    else:
        level = "Th·∫•p"

    logger.info(
        "FINAL -> Skill: %.2f | Req: %.2f | Desc: %.2f | AI: %.2f | Level: %s",
        skill_score, req_score, desc_score, ai_score, level
    )

    return skill_score, req_score, ai_score, level
